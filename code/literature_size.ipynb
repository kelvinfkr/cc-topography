{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_pdf = True\n",
    "import matplotlib\n",
    "if mp_pdf:\n",
    "    matplotlib.use('pgf')   \n",
    "    pgf_with_latex = {\n",
    "        \"text.usetex\": True,            # use LaTeX to write all text\n",
    "        \"pgf.rcfonts\": False,           # Ignore Matplotlibrc\n",
    "        \"text.latex.unicode\": True,\n",
    "        \"pgf.preamble\": [\n",
    "            #r\"\\usepackage[utf8x]{inputenc}\",\n",
    "            r\"\\usepackage{xcolor}\"\n",
    "        ],\n",
    "        \"pgf.texsystem\" : \"xelatex\",\n",
    "        \"figure.figsize\": [3.5,5]\n",
    "    }\n",
    "    matplotlib.rcParams.update(pgf_with_latex)\n",
    "    extension=\"pdf\"\n",
    "else:\n",
    "    extension=\"png\"\n",
    "    \n",
    "import os, sys, time, resource, re, gc, shutil\n",
    "from multiprocess import Pool\n",
    "from functools import partial\n",
    "from urllib.parse import urlparse, parse_qsl\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import django\n",
    "import pickle\n",
    "from django.db.models import Count, Sum\n",
    "sys.path.append('/home/galm/software/django/tmv/BasicBrowser/')\n",
    "os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"BasicBrowser.settings\")\n",
    "django.setup()\n",
    "\n",
    "from scoping.models import *\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = Query.objects.get(pk=6187)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stoplist = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "stoplist.add('elsevier')\n",
    "stoplist.add('rights')\n",
    "stoplist.add('reserved')\n",
    "stoplist.add('john')\n",
    "stoplist.add('wiley')\n",
    "stoplist.add('sons')\n",
    "stoplist.add('copyright')\n",
    "\n",
    "stopwords = stoplist\n",
    "\n",
    "def lemmatize(token, tag):\n",
    "        tag = {\n",
    "            'N': wn.NOUN,\n",
    "            'V': wn.VERB,\n",
    "            'R': wn.ADV,\n",
    "            'J': wn.ADJ\n",
    "        }.get(tag[0], wn.NOUN)\n",
    "        return WordNetLemmatizer().lemmatize(token, tag)\n",
    "\n",
    "kws = Doc.objects.filter(\n",
    "    query=q,\n",
    "    kw__text__iregex='\\W'\n",
    ").values('kw__text').annotate(\n",
    "    n = Count('pk')\n",
    ").filter(n__gt=100).order_by('-n')\n",
    "\n",
    "kw_text = set([x['kw__text'].replace('-',' ') for x in kws])\n",
    "kw_ws = set([x['kw__text'].replace('-',' ').split()[0] for x in kws]) - stopwords\n",
    "\n",
    "def tokenize(X):\n",
    "    for sent in sent_tokenize(X):\n",
    "        for token, tag in pos_tag(wordpunct_tokenize(sent)):\n",
    "            token = token.lower().strip()\n",
    "            if token in stopwords:\n",
    "                continue\n",
    "            if all(char in punct for char in token):\n",
    "                continue\n",
    "            if len(token) < 3:\n",
    "                continue\n",
    "            if all(char in string.digits for char in token):\n",
    "                continue\n",
    "            lemma = lemmatize(token,tag)\n",
    "            yield lemma\n",
    "\n",
    "def fancy_tokenize(X):\n",
    "    common_words = set([x.lower() for x in X.split()]) & kw_ws\n",
    "    for w in list(common_words):\n",
    "        w = w.replace('(','').replace(')','')\n",
    "        wpat = \"({}\\W*\\w*)\".format(w)\n",
    "        wn = [x.lower().replace('-',' ') for x in re.findall(wpat, X, re.IGNORECASE)]\n",
    "        kw_matches = set(wn) & kw_text\n",
    "        if len(kw_matches) > 0:\n",
    "            for m in kw_matches:\n",
    "                insensitive_m = re.compile(m, re.IGNORECASE)\n",
    "                X = insensitive_m.sub(' ', X)\n",
    "                yield m.replace(\" \",\"-\")\n",
    "\n",
    "    for sent in sent_tokenize(X):\n",
    "        for token, tag in pos_tag(wordpunct_tokenize(sent)):\n",
    "            token = token.lower().strip()\n",
    "            if token in stopwords:\n",
    "                continue\n",
    "            if all(char in punct for char in token):\n",
    "                continue\n",
    "            if len(token) < 3:\n",
    "                continue\n",
    "            if all(char in string.digits for char in token):\n",
    "                continue\n",
    "            lemma = lemmatize(token,tag)\n",
    "            yield lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Container object of 2 artists>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = Query.objects.get(pk=6187)\n",
    "\n",
    "before = q.doc_set.filter(PY__lt=2014).count()\n",
    "\n",
    "since = q.doc_set.filter(PY__gte=2014).count()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.bar(\n",
    "    [1,2],\n",
    "    [before,since]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7,4))\n",
    "\n",
    "ax.bar(\n",
    "    [1,2],\n",
    "    [before,since],\n",
    "    edgecolor=\"#F0F0F0\",\n",
    "    color=\"#636363\",\n",
    "    linewidth=1\n",
    ")\n",
    "\n",
    "ax.set_xticks([1,2])\n",
    "ax.set_xticklabels([\"Vor 2014\", \"2014-2018\"])\n",
    "\n",
    "ax.set_title(\"Artikel Ã¼ber Klimawandel\")\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig(\"../plots/before_since_AR5.pdf\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AR1</td>\n",
       "      <td>1986-1989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AR2</td>\n",
       "      <td>1990-1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AR3</td>\n",
       "      <td>1995-2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AR4</td>\n",
       "      <td>2001-2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AR5</td>\n",
       "      <td>2007-2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AR6</td>\n",
       "      <td>2014-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  name      years\n",
       "0  AR1  1986-1989\n",
       "1  AR2  1990-1994\n",
       "2  AR3  1995-2000\n",
       "3  AR4  2001-2006\n",
       "4  AR5  2007-2013\n",
       "5  AR6      2014-"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ars = AR.objects.filter(ar__gt=0).order_by('ar').values('name','start','end')\n",
    "\n",
    "for ar in ars:\n",
    "    ar[\"years\"] = f'{ar[\"start\"]}-{ar[\"end\"]}'\n",
    "    if ar[\"name\"] == \"AR6\":\n",
    "        ar[\"years\"] = f'{ar[\"start\"]}-'\n",
    "\n",
    "ardf = pd.DataFrame.from_dict(list(ars))[['name','years']]\n",
    "\n",
    "ardf.to_latex(\"../tables/ar_years.tex\",index=False)\n",
    "#ardf['years'] = f\"-{ardf['start']}\"\n",
    "ardf.head(6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PY</th>\n",
       "      <th>n</th>\n",
       "      <th>AR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1900.0</td>\n",
       "      <td>1</td>\n",
       "      <td>AR0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1903.0</td>\n",
       "      <td>1</td>\n",
       "      <td>AR0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1904.0</td>\n",
       "      <td>1</td>\n",
       "      <td>AR0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1905.0</td>\n",
       "      <td>1</td>\n",
       "      <td>AR0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1907.0</td>\n",
       "      <td>1</td>\n",
       "      <td>AR0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PY  n   AR\n",
       "0  1900.0  1  AR0\n",
       "1  1903.0  1  AR0\n",
       "2  1904.0  1  AR0\n",
       "3  1905.0  1  AR0\n",
       "4  1907.0  1  AR0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bypy = Doc.objects.filter(query=q).values('PY').annotate(\n",
    "    n = Count('pk')\n",
    ")\n",
    "def get_ar(x):\n",
    "    for ar in AR.objects.order_by('ar'):\n",
    "        if x >= ar.start and x <= ar.end:\n",
    "            return ar.name\n",
    "\n",
    "pydf = pd.DataFrame.from_dict(list(bypy))\n",
    "pydf['AR'] = pydf['PY'].apply(lambda x: get_ar(x))\n",
    "pydf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "369547.9821679168\n"
     ]
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "\n",
    "colors = dict(zip(\n",
    "    AR.objects.filter(ar__gt=0).order_by('ar').values_list('name',flat=True),\n",
    "    ['#d53e4f','#fc8d59','#fee08b','#e6f598','#99d594','#3288bd']\n",
    "))\n",
    "\n",
    "\n",
    "pdf = pydf[pydf['PY']>1986].sort_values('PY').reset_index(drop=True)\n",
    "\n",
    "def CAGR(first, last, periods):\n",
    "    try:\n",
    "        return (last/first)**(1/periods)-1\n",
    "    except:\n",
    "        return np.nan\n",
    "    \n",
    "pdf['lag_5'] = pdf['n'].shift(5)\n",
    "pdf['CAGR_5'] = pdf.apply(lambda x: CAGR(x['lag_5'], x['n'] ,5), axis=1)\n",
    "\n",
    "\n",
    "ec = \"#636363\"\n",
    "for name, group in pdf.groupby('AR'):\n",
    "    group = group.reset_index(drop=True)\n",
    "    ax.bar(\n",
    "        group['PY'],\n",
    "        group['n'],\n",
    "        color=[colors[x] for x in group['AR']],\n",
    "        edgecolor=ec,\n",
    "        label=name\n",
    "    )\n",
    "    mid = group['PY'].median()\n",
    "    max_n = group['n'].max() + 800\n",
    "    sum_n = group['n'].sum()\n",
    "    if name==\"AR6\":\n",
    "        continue\n",
    "    ax.text(mid, max_n, f\"[{sum_n:,}]\", ha=\"center\", va=\"bottom\")\n",
    "    \n",
    "ax.set_ylim(0,max_n+5000)\n",
    "    \n",
    "last_recorded = pdf['n'].iloc[-1]\n",
    "last = last_recorded\n",
    "grate = pdf['CAGR_5'].iloc[-1]\n",
    "projections=0\n",
    "ar6_total = sum_n\n",
    "ar6_nogrowth = sum_n\n",
    "fyears = [2019, 2020, 2021]\n",
    "\n",
    "for py in fyears:\n",
    "    if py==2019:\n",
    "        name_ng = \"Projections (no growth)\"\n",
    "        name_cg = \"Projections (constant growth)\"\n",
    "    else:\n",
    "        name_ng = None\n",
    "        name_cg = None\n",
    "    ax.bar(\n",
    "        py,\n",
    "        last_recorded,\n",
    "        color = colors['AR6'],\n",
    "        edgecolor=ec,\n",
    "        label=name_ng,\n",
    "        alpha=0.75\n",
    "    )\n",
    "    growth = last*grate\n",
    "    projections+=growth\n",
    "    ax.bar(\n",
    "        py,\n",
    "        projections,\n",
    "        bottom=last_recorded,\n",
    "        color = colors['AR6'],\n",
    "        edgecolor=ec,\n",
    "        hatch=\"//\",\n",
    "        label=name_cg,\n",
    "        alpha=0.75\n",
    "    )    \n",
    "    last = last+growth\n",
    "    max_n = last\n",
    "    ar6_total +=last\n",
    "    ar6_nogrowth +=last_recorded\n",
    "    #new = last*pdf['CAGR_5'][-1]\n",
    "    \n",
    "print(ar6_total)\n",
    "\n",
    "mid = pd.concat([group['PY'], pd.Series(fyears)]).median()\n",
    "label = f\"[{ar6_nogrowth:,}-\\n{int(round(ar6_total)):,}]\"\n",
    "ax.text(mid, max_n, label, ha=\"center\", va=\"bottom\")\n",
    "            \n",
    "ax.set_ylim(0,max_n+10000)\n",
    "ax.grid(linestyle='-', linewidth=0.5,axis=\"y\")\n",
    "\n",
    "ax.set_xlabel(\"Year\")\n",
    "ax.set_ylabel(\"Number of Publications\")\n",
    "\n",
    "plt.legend()\n",
    "            \n",
    "fig.patch.set_facecolor('#f0f0f0') \n",
    "    \n",
    "plt.savefig(\n",
    "    f'../plots/literature_size/pubs_time.{extension}',\n",
    "    bbox_inches='tight',\n",
    "    facecolor=fig.get_facecolor(),\n",
    "    pad_inches=0.2\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../tables/sizes_X.pickle\", \"rb\") as f:\n",
    "    X = pickle.load(f)\n",
    "    \n",
    "with open(\"../tables/vecs.pickle\", \"rb\") as f:\n",
    "    vecs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-10506a35be1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../tables/sizes_f_X.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mf_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../tables/f_vecs.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mf_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "with open(\"../tables/sizes_f_X.pickle\", \"rb\") as f:\n",
    "    f_X = pickle.load(f)\n",
    "    \n",
    "with open(\"../tables/f_vecs.pickle\", \"rb\") as f:\n",
    "    f_vecs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vocab = set()\n",
    "ignore_vocab = set([\n",
    "    'tildeo','2degreesc','5degreesc','1degreesc','0degreesc','5degrees',\n",
    "    'to10','3degreesc','4degreesc','5parts','8degreesc'\n",
    "])\n",
    "new_vocabs = []\n",
    "for i, vec in enumerate(vecs):\n",
    "    x = X[i]\n",
    "    vocab = vec.vocabulary_\n",
    "    sum_words = x.sum(axis=0).tolist()[0]\n",
    "    new_vocab = [(x[0],sum_words[x[1]]) for x in vocab.items() if x[0] not in all_vocab and x[0] not in ignore_vocab]\n",
    "    new_vocab =sorted(new_vocab, key = lambda x: x[1], reverse=True)\n",
    "    all_vocab = all_vocab | set(vec.get_feature_names()) - ignore_vocab\n",
    "    new_vocabs.append(new_vocab[:10])\n",
    "    print(new_vocab[:20])\n",
    "    print('\\n')\n",
    "\n",
    "tfmt = \" \".join([\"p{2cm}\" for x in range(7)])\n",
    "tfmt = \" \".join([\"l\" for x in range(7)])\n",
    "tfmt = \"l \" + \" \".join([\"p{1.8cm}\" for x in range(6)])\n",
    "hrow = \"&\" + \" & \".join([f\"\\\\textbf{{{x.name}}}\" for x in AR.objects.filter(ar__gt=0).order_by('ar')]) + \"\\\\\\\\ \\hline\"\n",
    "drow = \"\\\\textbf{Documents} &\" +\" & \".join([str(x.shape[0]) for x in X]) + \"\\\\\\\\ \\n\"\n",
    "wrow = \"\\\\textbf{Words} &\" +\" & \".join([str(x.shape[1]) for x in X]) + \"\\\\\\\\ \\n\"\n",
    "#nwrow = \"\\\\textbf{Words} &\" + \" & \".join([str(x) for x in new_vocabs]) \n",
    "nwrows = \"\\\\textbf{New words}\" + \"\\\\\\\\\".join([\" & \" + \" & \".join([f\"{x[i][0]} ({x[i][1]})\" for x in new_vocabs]) for i in range(8)])\n",
    "print(hrow) \n",
    "\n",
    "with open(\"../tables/growth_table.tex\",\"w\") as f:\n",
    "    f.write(\"\\\\begin{tabular}{\" + tfmt + \"} \\n\")\n",
    "    f.write(hrow)\n",
    "    f.write(drow)\n",
    "    f.write(wrow)\n",
    "    f.write(nwrows)\n",
    "    f.write(\"\\n\\\\end{tabular}\")\n",
    "## MMMs = Mixed matrix membrane\n",
    "nwrows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = new_vocabs[0]\n",
    "x[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## parse the bible, turn it into same format\n",
    "import codecs\n",
    "import re\n",
    "bible = []\n",
    "i=0\n",
    "clines=\"\"\n",
    "corpus = []\n",
    "lchapter=\"none\"\n",
    "with codecs.open('/home/galm/projects/big_literature/py/Martin_Luther_Uebersetzung_1912.txt',encoding = \"ISO-8859-1\") as ml:\n",
    "    for line in ml:\n",
    "        chapter = line.split(':')[0]\n",
    "        try:\n",
    "            verse = re.search(\".*([0-9]+\\:[0-9]*)(.*)\",line).group(2).strip()\n",
    "        except:\n",
    "            corpus.append(str(clines))\n",
    "            break\n",
    "        if lchapter!=chapter and i > 0:\n",
    "            corpus.append(str(clines))\n",
    "            clines=verse            \n",
    "        else:\n",
    "            clines+=\" \"+verse\n",
    "            #print('\\n')\n",
    "        lchapter = chapter\n",
    "        i+=1\n",
    "        \n",
    "X_y = c_vectorizer.fit_transform(corpus)\n",
    "X.append(X_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = np.arange(1)\n",
    "\n",
    "def vvrect(X,col_x,m):\n",
    "    return plt.bar(ind+X.shape[1]/2,X.shape[0],width=X.shape[1],facecolor=col_x,edgecolor='black',alpha=m)#,alpha=m)\n",
    "\n",
    "means = [x.getnnz(0).mean()/x.shape[0]*100 for x in X]\n",
    "nmeans = normalize([means])[0]\n",
    "\n",
    "crange = ['#fbb4ae','#b3cde3','#ccebc5','#decbe4','#fed9a6','#ffffcc']\n",
    "\n",
    "crange = [\"#D53E4F\", \"#FC8D59\", \"#FEE08B\", \"#E6F598\", \"#99D594\", \"#3288BD\"]\n",
    "\n",
    "crange = [\"#8dd3c7\",\"#ffffb3\",\"#bebada\",\"#fb8072\",\"#80b1d3\",\"#fdb462\",\"grey\"]\n",
    "\n",
    "years = [x.name for x in ars] + [\"Bible\"]\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "plots = []\n",
    "lvalues = []\n",
    "for x in [5,4,3,2,1,0,6]:\n",
    "    a = (nmeans[x]+0.3)/1.3\n",
    "    #a = nmeans[x]\n",
    "    vvrect(X[x], \"None\",1)\n",
    "    vvrect(X[x], \"white\",1)\n",
    "    p = vvrect(X[x], crange[x] ,a)\n",
    "    plots.append(p[0])\n",
    "    lvalues.append(years[x] + \": \" + str(round(means[x],2)))\n",
    "    print(crange[5-x])\n",
    "    print(a)\n",
    "    \n",
    "plt.legend(plots,lvalues,title='% of documents mean\\nterm appears in')\n",
    "    \n",
    "plt.axis('equal')\n",
    "plt.ylabel('Documents')\n",
    "plt.xlabel('Words')\n",
    "fig.patch.set_facecolor('#f0f0f0')    \n",
    "#plt.tight_layout()\n",
    "\n",
    "plt.savefig(\n",
    "    '../plots/literature_size/volume_variety.pdf',\n",
    "    bbox_inches='tight',\n",
    "    facecolor=fig.get_facecolor(),\n",
    ")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab = pd.DataFrame([(x.shape[0],x.shape[1]) for x in X]).rename({0:'Documents',1:'Terms'},axis=\"columns\")\n",
    "tab.index = years\n",
    "tab['Average Document score per Term'] = means\n",
    "tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shape = [(x+6000)/10000 for x in list(X[5].shape)[::1]]\n",
    "\n",
    "#plt.rcParams[\"figure.figsize\"] = tuple([round(x) for x in shape[::1]])\n",
    "\n",
    "#plt.rcParams[\"figure.figsize\"] = (9.5,13)\n",
    "\n",
    "xs = [5,4,3,2,1,0,6]\n",
    "\n",
    "for ar in AR.objects.all():\n",
    "    \n",
    "    iprs = IPCCRef.objects.filter(ar=ar).count()\n",
    "    \n",
    "    plots = []\n",
    "    lvalues = []\n",
    "\n",
    "    ar_xs = [x for x in xs if ar.ar > x ]\n",
    "    print(ar_xs)\n",
    "    #continue\n",
    "    for x in ar_xs:\n",
    "        a = (nmeans[x]+0.1)/1.1\n",
    "        #a = nmeans[x]\n",
    "        vvrect(X[x], \"None\",1)\n",
    "        vvrect(X[x], \"white\",1)\n",
    "        p = vvrect(X[x], crange[x] ,a)\n",
    "        plots.append(p[0])\n",
    "        lvalues.append(years[x] + \": \" + str(round(means[x],2)))\n",
    "        ar = AR.objects.get(ar=x+1)\n",
    "        iprs = IPCCRef.objects.filter(ar=ar).count()\n",
    "        xtent = shape[1]*10000+5000+1000\n",
    "        #plt.axhline(iprs,xmin=1/xtent*1000,xmax=1/xtent*(X[x].shape[1]+1000))\n",
    "    \n",
    "    plt.legend(plots,lvalues,title='% of documents mean term appears in')\n",
    "    \n",
    "    #plt.axis('equal')\n",
    "\n",
    "    #plt.ylim((-1000,130000))\n",
    "    #plt.ylim(ymin=-1000)\n",
    "    #plt.xlim((-1000,100000))\n",
    "    plt.xlim((-1000,shape[1]*10000+5000))\n",
    "    plt.ylim((-1000,shape[0]*10000+5000))\n",
    "    #plt.ylim(ymax=shape[0]*10000)\n",
    "    #plt.xlim(xmax=shape[1]*10000)\n",
    "    plt.ylabel('Documents')\n",
    "    plt.xlabel('Words')\n",
    "    plt.savefig('../plots/literature_size/volume_variety_{}.pdf'.format(ar.name),bbox_inches='tight')\n",
    "    \n",
    "    print(plt.xlim())\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ar in ars:\n",
    "    plots = []\n",
    "    lvalues = []\n",
    "\n",
    "    ar_xs = [x for x in xs if ar.ar > x or x ==6]\n",
    "    print(ar_xs)\n",
    "    #continue\n",
    "    for x in ar_xs:\n",
    "        a = (nmeans[x]+0.1)/1.1\n",
    "        #a = nmeans[x]\n",
    "        vvrect(X[x], \"None\",1)\n",
    "        vvrect(X[x], \"white\",1)\n",
    "        p = vvrect(X[x], crange[x] ,a)\n",
    "        plots.append(p[0])\n",
    "        #lvalues.append(years[x] + \": \" + str(round(X[x].mean()*1000000/X[x].sum(),5)))\n",
    "        lvalues.append(years[x] + \": \" + str(round(means[x],2)))\n",
    "        print(crange[5-x])\n",
    "        print(a)\n",
    "    \n",
    "    plt.legend(plots,lvalues,title='% of documents mean term appears in')\n",
    "    \n",
    "    #plt.axis('equal')\n",
    "\n",
    "    #plt.ylim((-1000,130000))\n",
    "    #plt.ylim(ymin=-1000)\n",
    "    #plt.xlim((-1000,100000))\n",
    "    plt.xlim((-1000,shape[1]*10000+5000))\n",
    "    plt.ylim((-1000,shape[0]*10000+5000))\n",
    "    #plt.ylim(ymax=shape[0]*10000)\n",
    "    #plt.xlim(xmax=shape[1]*10000)\n",
    "    plt.ylabel('Documents')\n",
    "    plt.xlabel('Words')\n",
    "    plt.savefig('../plots/literatures_size/volume_variety_bible_{}.pdf'.format(ar.name),bbox_inches='tight')\n",
    "    \n",
    "    print(plt.xlim())\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means\n",
    "\n",
    "nmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in xs:\n",
    "    print(X[x].sum(0).mean())\n",
    "    print(X[x].sum(0).mean()/X[x].shape[0]*100)\n",
    "    print(X[x].shape)\n",
    "    print('\\n\\n')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in xs:\n",
    "    print(X[x].getnnz(0).mean())\n",
    "    print(X[x].getnnz(0).mean()/X[x].shape[0]*100)\n",
    "    print(X[x].shape)\n",
    "    print('\\n\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_y.sum(0).mean()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tmv",
   "language": "python",
   "name": "tmv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
